loading llm model meta-llama/Llama-2-7b-chat-hf
use device  cuda:0
evaluating on wikitext2
nsamples 83
sample 0/83
sample 1/83
sample 2/83
sample 3/83
sample 4/83
sample 5/83
sample 6/83
sample 7/83
sample 8/83
sample 9/83
sample 10/83
sample 11/83
sample 12/83
sample 13/83
sample 14/83
sample 15/83
sample 16/83
sample 17/83
sample 18/83
sample 19/83
sample 20/83
sample 21/83
sample 22/83
sample 23/83
sample 24/83
sample 25/83
sample 26/83
sample 27/83
sample 28/83
sample 29/83
sample 30/83
sample 31/83
sample 32/83
sample 33/83
sample 34/83
sample 35/83
sample 36/83
sample 37/83
sample 38/83
sample 39/83
sample 40/83
sample 41/83
sample 42/83
sample 43/83
sample 44/83
sample 45/83
sample 46/83
sample 47/83
sample 48/83
sample 49/83
sample 50/83
sample 51/83
sample 52/83
sample 53/83
sample 54/83
sample 55/83
sample 56/83
sample 57/83
sample 58/83
sample 59/83
sample 60/83
sample 61/83
sample 62/83
sample 63/83
sample 64/83
sample 65/83
sample 66/83
sample 67/83
sample 68/83
sample 69/83
sample 70/83
sample 71/83
sample 72/83
sample 73/83
sample 74/83
sample 75/83
sample 76/83
sample 77/83
sample 78/83
sample 79/83
sample 80/83
sample 81/83
sample 82/83
args.prune_type  weight_thresold
weight_thresold_rank_pruning,weight_thresold_rank_pruning,weight_thresold_rank_pruning
{0: {'self_attn.q_proj': tensor([2.1692e+01, 1.4508e+01, 1.3002e+01,  ..., 1.0843e-05, 5.0048e-06,
        3.5373e-06], device='cuda:0'), 'self_attn.k_proj': tensor([1.8495e+01, 1.6609e+01, 1.3783e+01,  ..., 1.4191e-05, 2.3841e-06,
        9.2423e-07], device='cuda:0'), 'self_attn.v_proj': tensor([4.0276e+00, 3.7002e+00, 3.6290e+00,  ..., 2.9616e-04, 1.9063e-04,
        2.0081e-05], device='cuda:0'), 'self_attn.o_proj': tensor([4.3562e+00, 2.6613e+00, 2.4912e+00,  ..., 2.1728e-04, 1.3560e-04,
        4.5631e-05], device='cuda:0'), 'mlp.gate_proj': tensor([11.6709, 11.1569, 10.1079,  ...,  0.2219,  0.2148,  0.1816],
       device='cuda:0'), 'mlp.up_proj': tensor([10.5079,  9.9877,  9.3462,  ...,  0.2051,  0.1958,  0.1551],
       device='cuda:0'), 'mlp.down_proj': tensor([8.8316, 6.1055, 5.6116,  ..., 0.4784, 0.4614, 0.3921], device='cuda:0')}, 1: {'self_attn.q_proj': tensor([1.8268e+01, 1.4912e+01, 1.4254e+01,  ..., 4.6559e-05, 3.3616e-05,
        8.0495e-06], device='cuda:0'), 'self_attn.k_proj': tensor([1.9950e+01, 1.6137e+01, 1.3891e+01,  ..., 2.8334e-05, 1.6084e-05,
        7.5229e-06], device='cuda:0'), 'self_attn.v_proj': tensor([3.2673e+00, 3.0454e+00, 2.6937e+00,  ..., 4.6365e-04, 1.9069e-04,
        6.4804e-05], device='cuda:0'), 'self_attn.o_proj': tensor([3.8332e+00, 3.1084e+00, 2.2285e+00,  ..., 1.2698e-04, 1.0243e-04,
        2.5226e-05], device='cuda:0'), 'mlp.gate_proj': tensor([14.0056,  9.7365,  8.9175,  ...,  0.1821,  0.1597,  0.1312],
       device='cuda:0'), 'mlp.up_proj': tensor([7.2731, 4.6616, 4.6453,  ..., 0.1797, 0.1492, 0.1256], device='cuda:0'), 'mlp.down_proj': tensor([6.6077, 5.1402, 5.0573,  ..., 0.5416, 0.5075, 0.4543], device='cuda:0')}, 2: {'self_attn.q_proj': tensor([1.6130e+01, 1.1243e+01, 1.0049e+01,  ..., 1.6198e-04, 5.1200e-05,
        3.3184e-05], device='cuda:0'), 'self_attn.k_proj': tensor([1.9102e+01, 1.0712e+01, 9.9541e+00,  ..., 8.2509e-05, 3.6497e-05,
        2.2045e-05], device='cuda:0'), 'self_attn.v_proj': tensor([3.4908e+00, 3.3138e+00, 3.1692e+00,  ..., 5.8764e-04, 2.2367e-04,
        9.9307e-05], device='cuda:0'), 'self_attn.o_proj': tensor([5.9785e+00, 2.8748e+00, 2.7866e+00,  ..., 4.5299e-04, 2.8204e-04,
        1.0306e-04], device='cuda:0'), 'mlp.gate_proj': tensor([10.5383,  7.8919,  7.3435,  ...,  0.2455,  0.1794,  0.1402],
       device='cuda:0'), 'mlp.up_proj': tensor([4.5035, 4.2506, 4.1408,  ..., 0.2294, 0.1671, 0.1260], device='cuda:0'), 'mlp.down_proj': tensor([5.6278, 4.6495, 4.4858,  ..., 0.4204, 0.3930, 0.3770], device='cuda:0')}, 3: {'self_attn.q_proj': tensor([1.3306e+01, 8.7359e+00, 7.5359e+00,  ..., 3.0794e-04, 2.0191e-04,
        1.9076e-06], device='cuda:0'), 'self_attn.k_proj': tensor([1.5215e+01, 7.6340e+00, 6.8783e+00,  ..., 2.4932e-04, 1.3616e-04,
        5.3175e-05], device='cuda:0'), 'self_attn.v_proj': tensor([3.0334e+00, 2.8124e+00, 2.7255e+00,  ..., 4.2643e-04, 3.3949e-04,
        6.1176e-05], device='cuda:0'), 'self_attn.o_proj': tensor([6.3075e+00, 2.9532e+00, 2.7223e+00,  ..., 6.5401e-04, 3.7835e-04,
        3.3452e-04], device='cuda:0'), 'mlp.gate_proj': tensor([9.7766, 7.6884, 6.5288,  ..., 0.2965, 0.2918, 0.2197], device='cuda:0'), 'mlp.up_proj': tensor([4.3006, 4.1221, 3.9099,  ..., 0.2607, 0.2475, 0.1963], device='cuda:0'), 'mlp.down_proj': tensor([6.3007, 4.8410, 4.7638,  ..., 0.3994, 0.3879, 0.3629], device='cuda:0')}, 4: {'self_attn.q_proj': tensor([1.3684e+01, 9.1783e+00, 8.8870e+00,  ..., 3.9940e-04, 9.1623e-05,
        8.9861e-05], device='cuda:0'), 'self_attn.k_proj': tensor([1.5643e+01, 8.6863e+00, 8.1921e+00,  ..., 2.9183e-04, 1.7938e-04,
        6.5380e-05], device='cuda:0'), 'self_attn.v_proj': tensor([3.0754e+00, 3.0062e+00, 2.9165e+00,  ..., 6.4724e-04, 5.8369e-04,
        9.0062e-06], device='cuda:0'), 'self_attn.o_proj': tensor([5.7820e+00, 2.8989e+00, 2.7978e+00,  ..., 7.3388e-04, 5.6594e-04,
        1.1306e-04], device='cuda:0'), 'mlp.gate_proj': tensor([10.8390,  6.9564,  6.8614,  ...,  0.3422,  0.2990,  0.1899],
       device='cuda:0'), 'mlp.up_proj': tensor([4.2688, 3.9645, 3.8202,  ..., 0.3111, 0.2735, 0.1654], device='cuda:0'), 'mlp.down_proj': tensor([6.2328, 5.0403, 4.7703,  ..., 0.4554, 0.4086, 0.3917], device='cuda:0')}, 5: {'self_attn.q_proj': tensor([1.3639e+01, 8.9512e+00, 8.8049e+00,  ..., 4.2134e-04, 2.6710e-04,
        2.7462e-05], device='cuda:0'), 'self_attn.k_proj': tensor([1.6424e+01, 8.9628e+00, 8.5756e+00,  ..., 2.7571e-04, 1.5139e-04,
        1.3380e-04], device='cuda:0'), 'self_attn.v_proj': tensor([2.9723e+00, 2.9402e+00, 2.8182e+00,  ..., 4.0696e-04, 1.9470e-04,
        1.5953e-05], device='cuda:0'), 'self_attn.o_proj': tensor([5.6170e+00, 3.0367e+00, 2.8793e+00,  ..., 5.5046e-04, 3.1685e-04,
        1.2787e-05], device='cuda:0'), 'mlp.gate_proj': tensor([12.2529,  7.3572,  7.2315,  ...,  0.3603,  0.3287,  0.2722],
       device='cuda:0'), 'mlp.up_proj': tensor([4.4377, 4.1307, 4.0105,  ..., 0.3184, 0.3046, 0.2143], device='cuda:0'), 'mlp.down_proj': tensor([6.3831, 5.2935, 4.8846,  ..., 0.3848, 0.3459, 0.3379], device='cuda:0')}, 6: {'self_attn.q_proj': tensor([1.2224e+01, 8.1815e+00, 7.9923e+00,  ..., 6.2328e-04, 2.0042e-04,
        1.6091e-04], device='cuda:0'), 'self_attn.k_proj': tensor([1.4508e+01, 7.8972e+00, 7.5145e+00,  ..., 4.8664e-04, 4.3826e-04,
        4.7768e-06], device='cuda:0'), 'self_attn.v_proj': tensor([2.9797e+00, 2.9241e+00, 2.8498e+00,  ..., 5.2928e-04, 2.0932e-04,
        1.0370e-04], device='cuda:0'), 'self_attn.o_proj': tensor([5.7192e+00, 2.9850e+00, 2.9557e+00,  ..., 3.2577e-04, 1.9276e-04,
        9.7321e-05], device='cuda:0'), 'mlp.gate_proj': tensor([13.8324,  7.5427,  7.1948,  ...,  0.3794,  0.3078,  0.2888],
       device='cuda:0'), 'mlp.up_proj': tensor([4.7247, 4.4917, 4.1476,  ..., 0.3417, 0.2919, 0.2687], device='cuda:0'), 'mlp.down_proj': tensor([6.2394, 6.0888, 4.9463,  ..., 0.3601, 0.3385, 0.2790], device='cuda:0')}, 7: {'self_attn.q_proj': tensor([1.1693e+01, 7.6624e+00, 7.5004e+00,  ..., 4.7408e-04, 3.0050e-04,
        3.5848e-05], device='cuda:0'), 'self_attn.k_proj': tensor([1.3615e+01, 7.4819e+00, 7.1450e+00,  ..., 6.2473e-04, 4.9916e-04,
        1.9028e-04], device='cuda:0'), 'self_attn.v_proj': tensor([3.1062e+00, 3.0267e+00, 2.9696e+00,  ..., 4.2512e-04, 2.2795e-04,
        7.9264e-06], device='cuda:0'), 'self_attn.o_proj': tensor([5.4120e+00, 2.6893e+00, 2.6759e+00,  ..., 6.5909e-04, 3.3709e-04,
        1.2448e-04], device='cuda:0'), 'mlp.gate_proj': tensor([13.6891,  7.2789,  7.0650,  ...,  0.3993,  0.3784,  0.3405],
       device='cuda:0'), 'mlp.up_proj': tensor([4.9645, 4.2605, 4.2418,  ..., 0.4111, 0.3612, 0.3070], device='cuda:0'), 'mlp.down_proj': tensor([6.6761, 5.2532, 5.0288,  ..., 0.4018, 0.3886, 0.3588], device='cuda:0')}, 8: {'self_attn.q_proj': tensor([1.1897e+01, 7.8491e+00, 7.7385e+00,  ..., 6.0597e-04, 4.5377e-04,
        1.2414e-04], device='cuda:0'), 'self_attn.k_proj': tensor([1.4193e+01, 7.6329e+00, 7.4550e+00,  ..., 6.6987e-04, 3.4099e-04,
        8.3344e-05], device='cuda:0'), 'self_attn.v_proj': tensor([3.3437e+00, 3.2352e+00, 3.1755e+00,  ..., 4.3900e-04, 3.1784e-04,
        9.1573e-06], device='cuda:0'), 'self_attn.o_proj': tensor([4.8983e+00, 2.7935e+00, 2.7620e+00,  ..., 7.1368e-04, 4.4476e-04,
        1.2992e-04], device='cuda:0'), 'mlp.gate_proj': tensor([12.8530,  7.4247,  6.9729,  ...,  0.4070,  0.3636,  0.3280],
       device='cuda:0'), 'mlp.up_proj': tensor([5.3527, 4.6276, 4.5123,  ..., 0.3908, 0.3554, 0.3146], device='cuda:0'), 'mlp.down_proj': tensor([6.9947, 5.5090, 5.3101,  ..., 0.4126, 0.4023, 0.3688], device='cuda:0')}, 9: {'self_attn.q_proj': tensor([1.2148e+01, 7.7021e+00, 7.6871e+00,  ..., 5.4090e-04, 2.2200e-04,
        1.6237e-04], device='cuda:0'), 'self_attn.k_proj': tensor([1.4115e+01, 7.6334e+00, 7.4568e+00,  ..., 6.0811e-04, 2.3679e-04,
        9.9223e-05], device='cuda:0'), 'self_attn.v_proj': tensor([3.2050e+00, 3.1200e+00, 3.0251e+00,  ..., 8.3915e-04, 5.9541e-04,
        1.3684e-04], device='cuda:0'), 'self_attn.o_proj': tensor([5.0784e+00, 2.9049e+00, 2.8841e+00,  ..., 4.9967e-04, 3.1815e-04,
        1.5652e-04], device='cuda:0'), 'mlp.gate_proj': tensor([12.9473,  7.3268,  6.8430,  ...,  0.3819,  0.3603,  0.3198],
       device='cuda:0'), 'mlp.up_proj': tensor([5.3609, 4.8417, 4.7380,  ..., 0.3692, 0.3465, 0.3333], device='cuda:0'), 'mlp.down_proj': tensor([7.0308, 5.8747, 5.2565,  ..., 0.4347, 0.4310, 0.4079], device='cuda:0')}, 10: {'self_attn.q_proj': tensor([1.2008e+01, 8.3036e+00, 8.1197e+00,  ..., 8.3910e-04, 6.9004e-04,
        8.3988e-05], device='cuda:0'), 'self_attn.k_proj': tensor([1.4200e+01, 8.5177e+00, 7.9245e+00,  ..., 5.6558e-04, 2.5483e-04,
        2.2141e-04], device='cuda:0'), 'self_attn.v_proj': tensor([3.1927e+00, 3.1436e+00, 3.0993e+00,  ..., 7.4809e-04, 5.2557e-04,
        2.6868e-04], device='cuda:0'), 'self_attn.o_proj': tensor([4.2647e+00, 3.0879e+00, 3.0425e+00,  ..., 5.3335e-04, 2.5656e-04,
        1.0795e-04], device='cuda:0'), 'mlp.gate_proj': tensor([13.1091,  7.4579,  6.9239,  ...,  0.4004,  0.3819,  0.3068],
       device='cuda:0'), 'mlp.up_proj': tensor([5.5799, 4.9722, 4.8756,  ..., 0.4007, 0.3739, 0.3109], device='cuda:0'), 'mlp.down_proj': tensor([7.0067, 6.2462, 5.6067,  ..., 0.4137, 0.3542, 0.2735], device='cuda:0')}, 11: {'self_attn.q_proj': tensor([1.1114e+01, 7.8589e+00, 7.7046e+00,  ..., 8.2661e-04, 6.0710e-04,
        1.4597e-04], device='cuda:0'), 'self_attn.k_proj': tensor([1.2849e+01, 7.5878e+00, 7.4245e+00,  ..., 4.4818e-04, 1.7351e-04,
        5.5459e-06], device='cuda:0'), 'self_attn.v_proj': tensor([3.3611e+00, 3.2654e+00, 3.1728e+00,  ..., 5.3214e-04, 2.3241e-04,
        1.4731e-04], device='cuda:0'), 'self_attn.o_proj': tensor([5.5912e+00, 3.0482e+00, 3.0265e+00,  ..., 5.7136e-04, 1.9074e-04,
        1.0978e-04], device='cuda:0'), 'mlp.gate_proj': tensor([13.4796,  7.3236,  6.7315,  ...,  0.3311,  0.3087,  0.2984],
       device='cuda:0'), 'mlp.up_proj': tensor([5.6552, 4.9364, 4.7660,  ..., 0.3206, 0.3143, 0.3064], device='cuda:0'), 'mlp.down_proj': tensor([7.3901, 5.6287, 5.3958,  ..., 0.4335, 0.4260, 0.4022], device='cuda:0')}, 12: {'self_attn.q_proj': tensor([1.1723e+01, 7.5104e+00, 7.2629e+00,  ..., 8.6691e-04, 3.1723e-04,
        1.8921e-04], device='cuda:0'), 'self_attn.k_proj': tensor([1.3505e+01, 7.7517e+00, 7.6186e+00,  ..., 6.2307e-04, 4.5529e-04,
        7.3659e-05], device='cuda:0'), 'self_attn.v_proj': tensor([3.2061e+00, 3.1081e+00, 3.0772e+00,  ..., 4.2461e-04, 2.3883e-04,
        4.6198e-05], device='cuda:0'), 'self_attn.o_proj': tensor([4.9035e+00, 3.0217e+00, 2.9663e+00,  ..., 6.6266e-04, 2.6918e-04,
        9.9087e-05], device='cuda:0'), 'mlp.gate_proj': tensor([12.9885,  7.1684,  6.5597,  ...,  0.3334,  0.3152,  0.2761],
       device='cuda:0'), 'mlp.up_proj': tensor([5.9845, 4.8728, 4.7450,  ..., 0.3443, 0.3286, 0.2740], device='cuda:0'), 'mlp.down_proj': tensor([6.7554, 5.6954, 5.5008,  ..., 0.4150, 0.3817, 0.2546], device='cuda:0')}, 13: {'self_attn.q_proj': tensor([1.2018e+01, 7.2091e+00, 7.0668e+00,  ..., 5.7204e-04, 2.1337e-04,
        5.2999e-05], device='cuda:0'), 'self_attn.k_proj': tensor([1.3733e+01, 7.1613e+00, 7.1120e+00,  ..., 8.3645e-04, 3.1557e-04,
        1.0352e-04], device='cuda:0'), 'self_attn.v_proj': tensor([3.0825e+00, 2.9480e+00, 2.9200e+00,  ..., 6.4523e-04, 2.7221e-04,
        1.9574e-04], device='cuda:0'), 'self_attn.o_proj': tensor([5.0679e+00, 3.0778e+00, 3.0343e+00,  ..., 4.7243e-04, 4.0082e-04,
        7.8932e-05], device='cuda:0'), 'mlp.gate_proj': tensor([13.2010,  7.0280,  6.7605,  ...,  0.3113,  0.3078,  0.2670],
       device='cuda:0'), 'mlp.up_proj': tensor([6.3184, 4.9014, 4.7771,  ..., 0.3159, 0.3072, 0.2938], device='cuda:0'), 'mlp.down_proj': tensor([6.3615, 5.5224, 5.4794,  ..., 0.4687, 0.4364, 0.3777], device='cuda:0')}, 14: {'self_attn.q_proj': tensor([1.1555e+01, 7.3381e+00, 7.0193e+00,  ..., 7.6478e-04, 5.5474e-04,
        1.2727e-04], device='cuda:0'), 'self_attn.k_proj': tensor([1.3567e+01, 8.1135e+00, 7.8560e+00,  ..., 5.2951e-04, 1.9535e-04,
        5.6082e-05], device='cuda:0'), 'self_attn.v_proj': tensor([2.7269e+00, 2.6448e+00, 2.6272e+00,  ..., 5.3235e-04, 4.2863e-04,
        3.0901e-04], device='cuda:0'), 'self_attn.o_proj': tensor([4.5107e+00, 2.9845e+00, 2.9595e+00,  ..., 5.1774e-04, 2.2130e-04,
        9.8061e-05], device='cuda:0'), 'mlp.gate_proj': tensor([13.0716,  6.7250,  6.4505,  ...,  0.3043,  0.2906,  0.2673],
       device='cuda:0'), 'mlp.up_proj': tensor([6.4796, 5.0147, 4.7995,  ..., 0.2963, 0.2934, 0.2889], device='cuda:0'), 'mlp.down_proj': tensor([6.0907, 5.9335, 5.6357,  ..., 0.4516, 0.3899, 0.3571], device='cuda:0')}, 15: {'self_attn.q_proj': tensor([1.2638e+01, 7.3491e+00, 7.2421e+00,  ..., 8.0593e-04, 1.4671e-04,
        2.3219e-05], device='cuda:0'), 'self_attn.k_proj': tensor([1.4211e+01, 7.8921e+00, 7.6412e+00,  ..., 6.1349e-04, 2.3435e-04,
        9.2093e-05], device='cuda:0'), 'self_attn.v_proj': tensor([2.9718e+00, 2.9461e+00, 2.9127e+00,  ..., 7.8998e-04, 1.3535e-04,
        1.5116e-05], device='cuda:0'), 'self_attn.o_proj': tensor([4.5493e+00, 3.0422e+00, 2.9872e+00,  ..., 5.4013e-04, 4.4112e-04,
        1.7350e-04], device='cuda:0'), 'mlp.gate_proj': tensor([13.0188,  6.7377,  6.5427,  ...,  0.3103,  0.2966,  0.2875],
       device='cuda:0'), 'mlp.up_proj': tensor([6.7945, 5.1596, 4.8412,  ..., 0.3500, 0.3403, 0.3037], device='cuda:0'), 'mlp.down_proj': tensor([5.7214, 5.4502, 5.3661,  ..., 0.5437, 0.5053, 0.3528], device='cuda:0')}, 16: {'self_attn.q_proj': tensor([1.2899e+01, 6.6432e+00, 6.4859e+00,  ..., 1.0452e-03, 4.3997e-04,
        3.0634e-05], device='cuda:0'), 'self_attn.k_proj': tensor([1.4565e+01, 7.7823e+00, 7.3624e+00,  ..., 8.3699e-04, 5.2462e-04,
        1.4943e-04], device='cuda:0'), 'self_attn.v_proj': tensor([2.8084e+00, 2.5983e+00, 2.5231e+00,  ..., 5.4068e-04, 3.2882e-04,
        1.0002e-04], device='cuda:0'), 'self_attn.o_proj': tensor([4.9720e+00, 2.6642e+00, 2.6203e+00,  ..., 9.7131e-04, 3.2609e-04,
        1.7835e-04], device='cuda:0'), 'mlp.gate_proj': tensor([13.4296,  6.8937,  6.6833,  ...,  0.2743,  0.2705,  0.2506],
       device='cuda:0'), 'mlp.up_proj': tensor([7.0650, 5.2736, 4.8616,  ..., 0.2840, 0.2728, 0.2629], device='cuda:0'), 'mlp.down_proj': tensor([5.4283, 5.0524, 4.8315,  ..., 0.4377, 0.3766, 0.3420], device='cuda:0')}, 17: {'self_attn.q_proj': tensor([1.2839e+01, 6.3110e+00, 6.0804e+00,  ..., 8.1953e-04, 6.0191e-04,
        4.1654e-04], device='cuda:0'), 'self_attn.k_proj': tensor([1.4295e+01, 7.4088e+00, 6.8605e+00,  ..., 1.0114e-03, 4.6704e-04,
        6.4581e-05], device='cuda:0'), 'self_attn.v_proj': tensor([2.8653e+00, 2.6282e+00, 2.6132e+00,  ..., 7.2544e-04, 4.6287e-04,
        1.6406e-04], device='cuda:0'), 'self_attn.o_proj': tensor([5.4077e+00, 2.7773e+00, 2.7333e+00,  ..., 8.0603e-04, 4.4451e-04,
        1.7388e-04], device='cuda:0'), 'mlp.gate_proj': tensor([13.8728,  6.8929,  6.6363,  ...,  0.2782,  0.2727,  0.2533],
       device='cuda:0'), 'mlp.up_proj': tensor([6.5249, 5.1409, 4.8133,  ..., 0.2733, 0.2654, 0.2345], device='cuda:0'), 'mlp.down_proj': tensor([5.3845, 4.7925, 4.5406,  ..., 0.3388, 0.2988, 0.2974], device='cuda:0')}, 18: {'self_attn.q_proj': tensor([1.2935e+01, 5.7546e+00, 5.5999e+00,  ..., 7.4586e-04, 3.2806e-04,
        5.3142e-05], device='cuda:0'), 'self_attn.k_proj': tensor([1.4002e+01, 6.6874e+00, 6.2931e+00,  ..., 1.0721e-03, 6.4095e-04,
        4.4566e-04], device='cuda:0'), 'self_attn.v_proj': tensor([2.8656e+00, 2.6440e+00, 2.5209e+00,  ..., 8.7831e-04, 2.4756e-04,
        1.6659e-04], device='cuda:0'), 'self_attn.o_proj': tensor([5.3941e+00, 2.7786e+00, 2.6841e+00,  ..., 7.8276e-04, 3.4656e-04,
        1.4719e-04], device='cuda:0'), 'mlp.gate_proj': tensor([13.5276,  6.4985,  6.1523,  ...,  0.2811,  0.2774,  0.2306],
       device='cuda:0'), 'mlp.up_proj': tensor([6.0545, 5.1233, 4.5865,  ..., 0.2760, 0.2701, 0.2686], device='cuda:0'), 'mlp.down_proj': tensor([5.3399, 4.7860, 4.6925,  ..., 0.3368, 0.3272, 0.2948], device='cuda:0')}, 19: {'self_attn.q_proj': tensor([1.2726e+01, 6.5904e+00, 6.3089e+00,  ..., 8.3081e-04, 5.6857e-04,
        1.7908e-04], device='cuda:0'), 'self_attn.k_proj': tensor([1.4145e+01, 7.3894e+00, 6.7726e+00,  ..., 7.4743e-04, 3.5244e-04,
        8.7301e-05], device='cuda:0'), 'self_attn.v_proj': tensor([2.7194e+00, 2.7029e+00, 2.6540e+00,  ..., 1.3040e-03, 9.1241e-04,
        3.9923e-05], device='cuda:0'), 'self_attn.o_proj': tensor([5.3231e+00, 2.7044e+00, 2.6278e+00,  ..., 9.0719e-04, 3.9657e-04,
        1.9240e-04], device='cuda:0'), 'mlp.gate_proj': tensor([12.6038,  6.4832,  6.0800,  ...,  0.2778,  0.2739,  0.2678],
       device='cuda:0'), 'mlp.up_proj': tensor([5.9164, 5.3890, 4.6297,  ..., 0.2774, 0.2739, 0.2652], device='cuda:0'), 'mlp.down_proj': tensor([5.5323, 4.8973, 4.8243,  ..., 0.3886, 0.3160, 0.3049], device='cuda:0')}, 20: {'self_attn.q_proj': tensor([1.3254e+01, 6.7003e+00, 6.3419e+00,  ..., 7.1814e-04, 4.9994e-04,
        8.0380e-05], device='cuda:0'), 'self_attn.k_proj': tensor([1.4137e+01, 7.4961e+00, 6.9530e+00,  ..., 4.9245e-04, 3.1220e-04,
        1.4347e-04], device='cuda:0'), 'self_attn.v_proj': tensor([2.7248e+00, 2.6882e+00, 2.6368e+00,  ..., 1.3080e-03, 1.0206e-03,
        1.2989e-04], device='cuda:0'), 'self_attn.o_proj': tensor([6.5341e+00, 2.8394e+00, 2.6723e+00,  ..., 9.7409e-04, 1.3313e-04,
        4.9635e-05], device='cuda:0'), 'mlp.gate_proj': tensor([12.6945,  6.5768,  6.4647,  ...,  0.2824,  0.2687,  0.2206],
       device='cuda:0'), 'mlp.up_proj': tensor([6.0969, 5.4728, 4.6384,  ..., 0.2748, 0.2635, 0.2218], device='cuda:0'), 'mlp.down_proj': tensor([5.1183, 4.9737, 4.6206,  ..., 0.3118, 0.3091, 0.2991], device='cuda:0')}, 21: {'self_attn.q_proj': tensor([1.2704e+01, 6.8072e+00, 6.0530e+00,  ..., 7.3322e-04, 4.3859e-04,
        9.5648e-05], device='cuda:0'), 'self_attn.k_proj': tensor([1.3272e+01, 6.7745e+00, 6.5364e+00,  ..., 6.1140e-04, 5.7637e-04,
        2.2351e-05], device='cuda:0'), 'self_attn.v_proj': tensor([3.1851e+00, 2.8703e+00, 2.8342e+00,  ..., 1.2358e-03, 7.4454e-04,
        2.9648e-04], device='cuda:0'), 'self_attn.o_proj': tensor([5.5877e+00, 2.9341e+00, 2.6996e+00,  ..., 9.7889e-04, 2.0018e-04,
        9.3028e-05], device='cuda:0'), 'mlp.gate_proj': tensor([12.0921,  6.4042,  6.1050,  ...,  0.2848,  0.2770,  0.2359],
       device='cuda:0'), 'mlp.up_proj': tensor([5.6775, 5.3061, 4.6206,  ..., 0.2790, 0.2664, 0.2369], device='cuda:0'), 'mlp.down_proj': tensor([4.8360, 4.6979, 4.6589,  ..., 0.3307, 0.3169, 0.3053], device='cuda:0')}, 22: {'self_attn.q_proj': tensor([1.2881e+01, 6.5173e+00, 5.7295e+00,  ..., 9.1087e-04, 2.5005e-04,
        1.4081e-04], device='cuda:0'), 'self_attn.k_proj': tensor([1.3313e+01, 6.7614e+00, 6.4725e+00,  ..., 1.0721e-03, 3.9056e-04,
        3.1426e-04], device='cuda:0'), 'self_attn.v_proj': tensor([2.6971e+00, 2.6882e+00, 2.6707e+00,  ..., 1.1656e-03, 5.1751e-04,
        1.0582e-04], device='cuda:0'), 'self_attn.o_proj': tensor([7.5360e+00, 2.8361e+00, 2.6865e+00,  ..., 5.6625e-04, 3.0960e-04,
        1.8192e-04], device='cuda:0'), 'mlp.gate_proj': tensor([11.8855,  6.5571,  6.1494,  ...,  0.2876,  0.2746,  0.2552],
       device='cuda:0'), 'mlp.up_proj': tensor([5.2803, 5.2014, 4.8699,  ..., 0.2791, 0.2708, 0.2380], device='cuda:0'), 'mlp.down_proj': tensor([5.2240, 4.9907, 4.6018,  ..., 0.3228, 0.3129, 0.3094], device='cuda:0')}, 23: {'self_attn.q_proj': tensor([1.2414e+01, 5.6341e+00, 5.3431e+00,  ..., 6.3175e-04, 4.8336e-04,
        2.5700e-04], device='cuda:0'), 'self_attn.k_proj': tensor([1.2801e+01, 5.7869e+00, 5.6195e+00,  ..., 8.1993e-04, 4.5641e-04,
        1.4811e-05], device='cuda:0'), 'self_attn.v_proj': tensor([2.9949e+00, 2.9716e+00, 2.9612e+00,  ..., 9.9402e-04, 6.7233e-04,
        5.2165e-05], device='cuda:0'), 'self_attn.o_proj': tensor([5.4728e+00, 2.8104e+00, 2.7280e+00,  ..., 9.9703e-04, 3.1953e-04,
        1.0412e-04], device='cuda:0'), 'mlp.gate_proj': tensor([10.8868,  6.3131,  5.8093,  ...,  0.2957,  0.2824,  0.2380],
       device='cuda:0'), 'mlp.up_proj': tensor([4.8749, 4.7441, 4.5768,  ..., 0.2842, 0.2753, 0.2400], device='cuda:0'), 'mlp.down_proj': tensor([5.7946, 5.5321, 5.2964,  ..., 0.3281, 0.3210, 0.3126], device='cuda:0')}, 24: {'self_attn.q_proj': tensor([1.2307e+01, 7.3098e+00, 6.3796e+00,  ..., 5.7171e-04, 5.2462e-04,
        8.0478e-05], device='cuda:0'), 'self_attn.k_proj': tensor([1.3101e+01, 7.2179e+00, 6.9318e+00,  ..., 7.8517e-04, 3.1278e-04,
        1.6217e-04], device='cuda:0'), 'self_attn.v_proj': tensor([3.1814e+00, 3.0728e+00, 3.0046e+00,  ..., 8.5332e-04, 3.1752e-04,
        4.1601e-05], device='cuda:0'), 'self_attn.o_proj': tensor([6.6317e+00, 2.7698e+00, 2.6073e+00,  ..., 9.6319e-04, 3.8798e-04,
        8.0574e-05], device='cuda:0'), 'mlp.gate_proj': tensor([10.8502,  5.9406,  5.6488,  ...,  0.2914,  0.2858,  0.2363],
       device='cuda:0'), 'mlp.up_proj': tensor([4.6538, 4.4173, 4.3103,  ..., 0.2876, 0.2777, 0.2385], device='cuda:0'), 'mlp.down_proj': tensor([5.5497, 4.7535, 4.1843,  ..., 0.3484, 0.3430, 0.3012], device='cuda:0')}, 25: {'self_attn.q_proj': tensor([1.1753e+01, 5.8303e+00, 5.5951e+00,  ..., 8.5299e-04, 6.3249e-04,
        4.2868e-04], device='cuda:0'), 'self_attn.k_proj': tensor([1.2207e+01, 5.7199e+00, 5.4848e+00,  ..., 5.2278e-04, 3.0517e-04,
        1.0586e-04], device='cuda:0'), 'self_attn.v_proj': tensor([3.1564e+00, 3.0947e+00, 3.0407e+00,  ..., 9.3112e-04, 3.6436e-04,
        1.6113e-04], device='cuda:0'), 'self_attn.o_proj': tensor([4.8125e+00, 2.8782e+00, 2.8337e+00,  ..., 8.8680e-04, 4.4671e-04,
        2.5173e-04], device='cuda:0'), 'mlp.gate_proj': tensor([10.7875,  6.0123,  5.5495,  ...,  0.2973,  0.2914,  0.2483],
       device='cuda:0'), 'mlp.up_proj': tensor([4.8991, 4.2093, 4.1404,  ..., 0.2945, 0.2887, 0.2638], device='cuda:0'), 'mlp.down_proj': tensor([5.2587, 5.1205, 4.6242,  ..., 0.3693, 0.3564, 0.3015], device='cuda:0')}, 26: {'self_attn.q_proj': tensor([1.2785e+01, 6.3890e+00, 5.9919e+00,  ..., 4.4832e-04, 3.3445e-04,
        1.1188e-04], device='cuda:0'), 'self_attn.k_proj': tensor([1.3201e+01, 6.9349e+00, 6.5841e+00,  ..., 6.1670e-04, 4.8877e-04,
        1.8841e-05], device='cuda:0'), 'self_attn.v_proj': tensor([3.6150e+00, 3.5619e+00, 3.5502e+00,  ..., 7.4747e-04, 5.1277e-04,
        2.8838e-05], device='cuda:0'), 'self_attn.o_proj': tensor([6.8110e+00, 3.0043e+00, 2.9873e+00,  ..., 9.2644e-04, 3.5645e-04,
        1.6957e-05], device='cuda:0'), 'mlp.gate_proj': tensor([11.2170,  6.9724,  5.6220,  ...,  0.3031,  0.2981,  0.2922],
       device='cuda:0'), 'mlp.up_proj': tensor([5.7768, 4.1247, 4.0781,  ..., 0.3020, 0.2951, 0.2569], device='cuda:0'), 'mlp.down_proj': tensor([4.9493, 4.7527, 4.6994,  ..., 0.3979, 0.3810, 0.3173], device='cuda:0')}, 27: {'self_attn.q_proj': tensor([1.3034e+01, 6.3083e+00, 5.8124e+00,  ..., 1.1576e-03, 8.5062e-04,
        2.6245e-04], device='cuda:0'), 'self_attn.k_proj': tensor([1.3888e+01, 6.0042e+00, 5.7515e+00,  ..., 9.9875e-04, 6.9548e-04,
        8.7798e-05], device='cuda:0'), 'self_attn.v_proj': tensor([3.2921e+00, 3.0318e+00, 2.9524e+00,  ..., 1.2090e-03, 7.4973e-04,
        1.0892e-04], device='cuda:0'), 'self_attn.o_proj': tensor([5.6219e+00, 3.7131e+00, 3.0105e+00,  ..., 1.7061e-03, 1.2218e-03,
        2.5396e-04], device='cuda:0'), 'mlp.gate_proj': tensor([11.6356,  7.5839,  5.7267,  ...,  0.3089,  0.3017,  0.2404],
       device='cuda:0'), 'mlp.up_proj': tensor([6.8931, 4.5083, 4.2331,  ..., 0.3101, 0.3012, 0.2549], device='cuda:0'), 'mlp.down_proj': tensor([5.1117, 5.1048, 5.0257,  ..., 0.4058, 0.3842, 0.3306], device='cuda:0')}, 28: {'self_attn.q_proj': tensor([1.2654e+01, 6.8014e+00, 5.8402e+00,  ..., 1.2367e-03, 6.6885e-04,
        2.5364e-04], device='cuda:0'), 'self_attn.k_proj': tensor([1.3409e+01, 6.0816e+00, 5.6558e+00,  ..., 7.8187e-04, 2.8990e-04,
        2.5537e-04], device='cuda:0'), 'self_attn.v_proj': tensor([3.6627e+00, 3.1352e+00, 3.1220e+00,  ..., 9.7448e-04, 7.4171e-04,
        9.3536e-05], device='cuda:0'), 'self_attn.o_proj': tensor([6.0454e+00, 3.6037e+00, 3.2613e+00,  ..., 1.0242e-03, 3.3624e-04,
        1.4194e-04], device='cuda:0'), 'mlp.gate_proj': tensor([11.6311,  8.3466,  6.1804,  ...,  0.3259,  0.3227,  0.2452],
       device='cuda:0'), 'mlp.up_proj': tensor([8.9314, 4.9002, 4.5324,  ..., 0.3343, 0.3222, 0.2575], device='cuda:0'), 'mlp.down_proj': tensor([4.8988, 4.8407, 4.7670,  ..., 0.4395, 0.4246, 0.3504], device='cuda:0')}, 29: {'self_attn.q_proj': tensor([1.3128e+01, 7.6922e+00, 7.0616e+00,  ..., 7.7101e-04, 5.5875e-04,
        1.8143e-04], device='cuda:0'), 'self_attn.k_proj': tensor([1.4049e+01, 6.8094e+00, 6.2971e+00,  ..., 7.4788e-04, 4.3352e-04,
        8.6607e-05], device='cuda:0'), 'self_attn.v_proj': tensor([4.7545e+00, 3.3343e+00, 3.2321e+00,  ..., 1.0002e-03, 5.2235e-04,
        1.1871e-04], device='cuda:0'), 'self_attn.o_proj': tensor([6.7995e+00, 4.0911e+00, 4.0324e+00,  ..., 1.2772e-03, 3.9020e-04,
        2.0516e-05], device='cuda:0'), 'mlp.gate_proj': tensor([12.2732,  9.2047,  6.5605,  ...,  0.3784,  0.3542,  0.2612],
       device='cuda:0'), 'mlp.up_proj': tensor([11.1548,  7.0163,  5.3688,  ...,  0.3920,  0.3541,  0.2670],
       device='cuda:0'), 'mlp.down_proj': tensor([4.3832, 4.3529, 4.2269,  ..., 0.5121, 0.4987, 0.3840], device='cuda:0')}, 30: {'self_attn.q_proj': tensor([1.2959e+01, 8.2957e+00, 6.6754e+00,  ..., 7.8284e-04, 7.0973e-04,
        1.3913e-04], device='cuda:0'), 'self_attn.k_proj': tensor([1.3664e+01, 6.4276e+00, 6.0136e+00,  ..., 5.7185e-04, 3.1257e-04,
        2.1614e-04], device='cuda:0'), 'self_attn.v_proj': tensor([3.7428e+00, 3.4037e+00, 3.0738e+00,  ..., 1.4687e-03, 6.3359e-04,
        3.4721e-04], device='cuda:0'), 'self_attn.o_proj': tensor([6.8155e+00, 4.1695e+00, 4.0061e+00,  ..., 9.9794e-04, 6.5353e-04,
        3.2365e-04], device='cuda:0'), 'mlp.gate_proj': tensor([19.5836, 11.5735, 10.3032,  ...,  0.4053,  0.3639,  0.2666],
       device='cuda:0'), 'mlp.up_proj': tensor([19.6929, 12.8992,  6.7491,  ...,  0.4172,  0.3789,  0.2905],
       device='cuda:0'), 'mlp.down_proj': tensor([4.4908, 4.4064, 4.3259,  ..., 0.5685, 0.5529, 0.4718], device='cuda:0')}, 31: {'self_attn.q_proj': tensor([1.4850e+01, 8.3542e+00, 7.5951e+00,  ..., 5.3207e-04, 1.3507e-04,
        4.4254e-05], device='cuda:0'), 'self_attn.k_proj': tensor([1.6295e+01, 8.3054e+00, 8.1123e+00,  ..., 3.4885e-04, 1.2878e-04,
        4.7773e-05], device='cuda:0'), 'self_attn.v_proj': tensor([3.9784e+00, 3.8003e+00, 3.5391e+00,  ..., 9.4963e-04, 6.3053e-04,
        2.4096e-04], device='cuda:0'), 'self_attn.o_proj': tensor([1.0455e+01, 6.6622e+00, 5.5057e+00,  ..., 8.4123e-04, 2.6912e-04,
        1.6553e-04], device='cuda:0'), 'mlp.gate_proj': tensor([20.0009, 13.7219, 10.2988,  ...,  0.6200,  0.6021,  0.3950],
       device='cuda:0'), 'mlp.up_proj': tensor([19.6003, 12.6833, 10.2559,  ...,  0.6419,  0.6376,  0.5816],
       device='cuda:0'), 'mlp.down_proj': tensor([7.7841, 6.3674, 5.9626,  ..., 0.6130, 0.5780, 0.5051], device='cuda:0')}}
layer0.self_attn.q_proj rank reduction: 		98.706 %
layer0.self_attn.k_proj rank reduction: 		97.998 %
layer0.self_attn.v_proj rank reduction: 		70.605 %
layer0.self_attn.o_proj rank reduction: 		85.986 %
layer0.mlp.gate_proj rank reduction: 		72.607 %
layer0.mlp.up_proj rank reduction: 		64.575 %
layer0.mlp.down_proj rank reduction: 		53.516 %
layer1.self_attn.q_proj rank reduction: 		91.113 %
layer1.self_attn.k_proj rank reduction: 		91.846 %
layer1.self_attn.v_proj rank reduction: 		58.203 %
layer1.self_attn.o_proj rank reduction: 		71.533 %
layer1.mlp.gate_proj rank reduction: 		75.537 %
layer1.mlp.up_proj rank reduction: 		33.472 %
layer1.mlp.down_proj rank reduction: 		30.688 %
layer2.self_attn.q_proj rank reduction: 		84.717 %
layer2.self_attn.k_proj rank reduction: 		87.354 %
layer2.self_attn.v_proj rank reduction: 		39.990 %
layer2.self_attn.o_proj rank reduction: 		60.669 %
layer2.mlp.gate_proj rank reduction: 		50.586 %
layer2.mlp.up_proj rank reduction: 		5.542 %
layer2.mlp.down_proj rank reduction: 		17.529 %
layer3.self_attn.q_proj rank reduction: 		78.101 %
layer3.self_attn.k_proj rank reduction: 		80.591 %
layer3.self_attn.v_proj rank reduction: 		35.327 %
layer3.self_attn.o_proj rank reduction: 		66.113 %
layer3.mlp.gate_proj rank reduction: 		39.917 %
layer3.mlp.up_proj rank reduction: 		3.394 %
layer3.mlp.down_proj rank reduction: 		26.050 %
layer4.self_attn.q_proj rank reduction: 		78.760 %
layer4.self_attn.k_proj rank reduction: 		81.738 %
layer4.self_attn.v_proj rank reduction: 		33.740 %
layer4.self_attn.o_proj rank reduction: 		59.180 %
layer4.mlp.gate_proj rank reduction: 		44.580 %
layer4.mlp.up_proj rank reduction: 		1.978 %
layer4.mlp.down_proj rank reduction: 		27.490 %
layer5.self_attn.q_proj rank reduction: 		78.125 %
layer5.self_attn.k_proj rank reduction: 		82.642 %
layer5.self_attn.v_proj rank reduction: 		32.300 %
layer5.self_attn.o_proj rank reduction: 		57.300 %
layer5.mlp.gate_proj rank reduction: 		58.057 %
layer5.mlp.up_proj rank reduction: 		4.175 %
layer5.mlp.down_proj rank reduction: 		26.929 %
layer6.self_attn.q_proj rank reduction: 		77.710 %
layer6.self_attn.k_proj rank reduction: 		82.080 %
layer6.self_attn.v_proj rank reduction: 		35.913 %
layer6.self_attn.o_proj rank reduction: 		63.086 %
layer6.mlp.gate_proj rank reduction: 		67.285 %
layer6.mlp.up_proj rank reduction: 		8.398 %
layer6.mlp.down_proj rank reduction: 		23.828 %
layer7.self_attn.q_proj rank reduction: 		75.464 %
layer7.self_attn.k_proj rank reduction: 		80.200 %
layer7.self_attn.v_proj rank reduction: 		37.793 %
layer7.self_attn.o_proj rank reduction: 		59.277 %
layer7.mlp.gate_proj rank reduction: 		68.726 %
layer7.mlp.up_proj rank reduction: 		11.987 %
layer7.mlp.down_proj rank reduction: 		31.519 %
layer8.self_attn.q_proj rank reduction: 		75.098 %
layer8.self_attn.k_proj rank reduction: 		79.834 %
layer8.self_attn.v_proj rank reduction: 		40.259 %
layer8.self_attn.o_proj rank reduction: 		54.224 %
layer8.mlp.gate_proj rank reduction: 		65.186 %
layer8.mlp.up_proj rank reduction: 		14.868 %
layer8.mlp.down_proj rank reduction: 		34.448 %
layer9.self_attn.q_proj rank reduction: 		74.658 %
layer9.self_attn.k_proj rank reduction: 		78.882 %
layer9.self_attn.v_proj rank reduction: 		38.379 %
layer9.self_attn.o_proj rank reduction: 		55.054 %
layer9.mlp.gate_proj rank reduction: 		66.479 %
layer9.mlp.up_proj rank reduction: 		15.771 %
layer9.mlp.down_proj rank reduction: 		36.035 %
layer10.self_attn.q_proj rank reduction: 		76.050 %
layer10.self_attn.k_proj rank reduction: 		80.029 %
layer10.self_attn.v_proj rank reduction: 		38.818 %
layer10.self_attn.o_proj rank reduction: 		50.073 %
layer10.mlp.gate_proj rank reduction: 		67.554 %
layer10.mlp.up_proj rank reduction: 		16.284 %
layer10.mlp.down_proj rank reduction: 		29.102 %
layer11.self_attn.q_proj rank reduction: 		76.367 %
layer11.self_attn.k_proj rank reduction: 		80.420 %
layer11.self_attn.v_proj rank reduction: 		40.454 %
layer11.self_attn.o_proj rank reduction: 		60.547 %
layer11.mlp.gate_proj rank reduction: 		69.312 %
layer11.mlp.up_proj rank reduction: 		16.455 %
layer11.mlp.down_proj rank reduction: 		36.743 %
layer12.self_attn.q_proj rank reduction: 		74.780 %
layer12.self_attn.k_proj rank reduction: 		79.224 %
layer12.self_attn.v_proj rank reduction: 		37.695 %
layer12.self_attn.o_proj rank reduction: 		52.979 %
layer12.mlp.gate_proj rank reduction: 		65.967 %
layer12.mlp.up_proj rank reduction: 		17.017 %
layer12.mlp.down_proj rank reduction: 		24.390 %
layer13.self_attn.q_proj rank reduction: 		76.001 %
layer13.self_attn.k_proj rank reduction: 		79.272 %
layer13.self_attn.v_proj rank reduction: 		33.301 %
layer13.self_attn.o_proj rank reduction: 		53.467 %
layer13.mlp.gate_proj rank reduction: 		66.504 %
layer13.mlp.up_proj rank reduction: 		19.092 %
layer13.mlp.down_proj rank reduction: 		25.342 %
layer14.self_attn.q_proj rank reduction: 		76.221 %
layer14.self_attn.k_proj rank reduction: 		81.299 %
layer14.self_attn.v_proj rank reduction: 		30.811 %
layer14.self_attn.o_proj rank reduction: 		49.438 %
layer14.mlp.gate_proj rank reduction: 		65.845 %
layer14.mlp.up_proj rank reduction: 		20.020 %
layer14.mlp.down_proj rank reduction: 		22.144 %
layer15.self_attn.q_proj rank reduction: 		78.418 %
layer15.self_attn.k_proj rank reduction: 		81.860 %
layer15.self_attn.v_proj rank reduction: 		31.323 %
layer15.self_attn.o_proj rank reduction: 		46.899 %
layer15.mlp.gate_proj rank reduction: 		65.454 %
layer15.mlp.up_proj rank reduction: 		21.851 %
layer15.mlp.down_proj rank reduction: 		17.529 %
layer16.self_attn.q_proj rank reduction: 		80.029 %
layer16.self_attn.k_proj rank reduction: 		83.374 %
layer16.self_attn.v_proj rank reduction: 		27.124 %
layer16.self_attn.o_proj rank reduction: 		46.240 %
layer16.mlp.gate_proj rank reduction: 		65.869 %
layer16.mlp.up_proj rank reduction: 		22.583 %
layer16.mlp.down_proj rank reduction: 		14.136 %
layer17.self_attn.q_proj rank reduction: 		78.003 %
layer17.self_attn.k_proj rank reduction: 		81.934 %
layer17.self_attn.v_proj rank reduction: 		26.782 %
layer17.self_attn.o_proj rank reduction: 		48.291 %
layer17.mlp.gate_proj rank reduction: 		67.383 %
layer17.mlp.up_proj rank reduction: 		17.773 %
layer17.mlp.down_proj rank reduction: 		12.256 %
layer18.self_attn.q_proj rank reduction: 		78.076 %
layer18.self_attn.k_proj rank reduction: 		80.591 %
layer18.self_attn.v_proj rank reduction: 		25.415 %
layer18.self_attn.o_proj rank reduction: 		45.264 %
layer18.mlp.gate_proj rank reduction: 		62.988 %
layer18.mlp.up_proj rank reduction: 		15.723 %
layer18.mlp.down_proj rank reduction: 		11.572 %
layer19.self_attn.q_proj rank reduction: 		78.857 %
layer19.self_attn.k_proj rank reduction: 		82.812 %
layer19.self_attn.v_proj rank reduction: 		24.463 %
layer19.self_attn.o_proj rank reduction: 		44.385 %
layer19.mlp.gate_proj rank reduction: 		58.374 %
layer19.mlp.up_proj rank reduction: 		14.526 %
layer19.mlp.down_proj rank reduction: 		12.842 %
layer20.self_attn.q_proj rank reduction: 		80.493 %
layer20.self_attn.k_proj rank reduction: 		82.446 %
layer20.self_attn.v_proj rank reduction: 		23.730 %
layer20.self_attn.o_proj rank reduction: 		51.245 %
layer20.mlp.gate_proj rank reduction: 		56.836 %
layer20.mlp.up_proj rank reduction: 		14.233 %
layer20.mlp.down_proj rank reduction: 		9.253 %
layer21.self_attn.q_proj rank reduction: 		78.906 %
layer21.self_attn.k_proj rank reduction: 		80.005 %
layer21.self_attn.v_proj rank reduction: 		27.100 %
layer21.self_attn.o_proj rank reduction: 		42.944 %
layer21.mlp.gate_proj rank reduction: 		53.076 %
layer21.mlp.up_proj rank reduction: 		12.256 %
layer21.mlp.down_proj rank reduction: 		8.130 %
layer22.self_attn.q_proj rank reduction: 		77.734 %
layer22.self_attn.k_proj rank reduction: 		78.394 %
layer22.self_attn.v_proj rank reduction: 		22.168 %
layer22.self_attn.o_proj rank reduction: 		56.641 %
layer22.mlp.gate_proj rank reduction: 		52.100 %
layer22.mlp.up_proj rank reduction: 		9.863 %
layer22.mlp.down_proj rank reduction: 		10.815 %
layer23.self_attn.q_proj rank reduction: 		74.683 %
layer23.self_attn.k_proj rank reduction: 		75.830 %
layer23.self_attn.v_proj rank reduction: 		23.413 %
layer23.self_attn.o_proj rank reduction: 		40.527 %
layer23.mlp.gate_proj rank reduction: 		44.434 %
layer23.mlp.up_proj rank reduction: 		6.982 %
layer23.mlp.down_proj rank reduction: 		14.795 %
layer24.self_attn.q_proj rank reduction: 		78.027 %
layer24.self_attn.k_proj rank reduction: 		80.249 %
layer24.self_attn.v_proj rank reduction: 		25.439 %
layer24.self_attn.o_proj rank reduction: 		48.608 %
layer24.mlp.gate_proj rank reduction: 		43.555 %
layer24.mlp.up_proj rank reduction: 		5.444 %
layer24.mlp.down_proj rank reduction: 		13.135 %
layer25.self_attn.q_proj rank reduction: 		73.608 %
layer25.self_attn.k_proj rank reduction: 		74.634 %
layer25.self_attn.v_proj rank reduction: 		23.755 %
layer25.self_attn.o_proj rank reduction: 		35.742 %
layer25.mlp.gate_proj rank reduction: 		42.920 %
layer25.mlp.up_proj rank reduction: 		7.080 %
layer25.mlp.down_proj rank reduction: 		11.572 %
layer26.self_attn.q_proj rank reduction: 		79.858 %
layer26.self_attn.k_proj rank reduction: 		80.469 %
layer26.self_attn.v_proj rank reduction: 		27.319 %
layer26.self_attn.o_proj rank reduction: 		48.218 %
layer26.mlp.gate_proj rank reduction: 		46.973 %
layer26.mlp.up_proj rank reduction: 		11.475 %
layer26.mlp.down_proj rank reduction: 		10.132 %
layer27.self_attn.q_proj rank reduction: 		77.026 %
layer27.self_attn.k_proj rank reduction: 		79.370 %
layer27.self_attn.v_proj rank reduction: 		23.706 %
layer27.self_attn.o_proj rank reduction: 		39.941 %
layer27.mlp.gate_proj rank reduction: 		46.997 %
layer27.mlp.up_proj rank reduction: 		18.188 %
layer27.mlp.down_proj rank reduction: 		11.499 %
layer28.self_attn.q_proj rank reduction: 		77.661 %
layer28.self_attn.k_proj rank reduction: 		79.395 %
layer28.self_attn.v_proj rank reduction: 		25.732 %
layer28.self_attn.o_proj rank reduction: 		42.285 %
layer28.mlp.gate_proj rank reduction: 		47.754 %
layer28.mlp.up_proj rank reduction: 		32.104 %
layer28.mlp.down_proj rank reduction: 		9.766 %
layer29.self_attn.q_proj rank reduction: 		82.349 %
layer29.self_attn.k_proj rank reduction: 		84.302 %
layer29.self_attn.v_proj rank reduction: 		33.545 %
layer29.self_attn.o_proj rank reduction: 		45.923 %
layer29.mlp.gate_proj rank reduction: 		52.319 %
layer29.mlp.up_proj rank reduction: 		47.900 %
layer29.mlp.down_proj rank reduction: 		7.178 %
layer30.self_attn.q_proj rank reduction: 		80.347 %
layer30.self_attn.k_proj rank reduction: 		81.616 %
layer30.self_attn.v_proj rank reduction: 		25.195 %
layer30.self_attn.o_proj rank reduction: 		44.897 %
layer30.mlp.gate_proj rank reduction: 		88.110 %
layer30.mlp.up_proj rank reduction: 		92.383 %
layer30.mlp.down_proj rank reduction: 		11.206 %
layer31.self_attn.q_proj rank reduction: 		88.501 %
layer31.self_attn.k_proj rank reduction: 		89.771 %
layer31.self_attn.v_proj rank reduction: 		30.493 %
layer31.self_attn.o_proj rank reduction: 		72.168 %
layer31.mlp.gate_proj rank reduction: 		89.429 %
layer31.mlp.up_proj rank reduction: 		94.312 %
layer31.mlp.down_proj rank reduction: 		37.964 %


 Total Rank Reduction: 50.001 %
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 53
New matrix has shape torch.Size([4096, 4096])
layer.0.self_attn.q_proj (53): 0.0035932925529778004
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 82
New matrix has shape torch.Size([4096, 4096])
layer.0.self_attn.k_proj (82): 0.0035124123096466064
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 1204
New matrix has shape torch.Size([4096, 4096])
layer.0.self_attn.v_proj (1204): 0.0027082283049821854
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 574
New matrix has shape torch.Size([4096, 4096])
layer.0.self_attn.o_proj (574): 0.0029039697255939245
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 1122
New matrix has shape torch.Size([11008, 4096])
layer.0.mlp.gate_proj (1122): 0.007029643747955561
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 1451
New matrix has shape torch.Size([11008, 4096])
layer.0.mlp.up_proj (1451): 0.0061701214872300625
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 1904
New matrix has shape torch.Size([4096, 11008])
layer.0.mlp.down_proj (1904): 0.005969379562884569
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 364
New matrix has shape torch.Size([4096, 4096])
layer.1.self_attn.q_proj (364): 0.007192906923592091
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 334
New matrix has shape torch.Size([4096, 4096])
layer.1.self_attn.k_proj (334): 0.00731987040489912
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 1712
New matrix has shape torch.Size([4096, 4096])
layer.1.self_attn.v_proj (1712): 0.002195193665102124
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 1166
New matrix has shape torch.Size([4096, 4096])
layer.1.self_attn.o_proj (1166): 0.0024854859802871943
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 1002
New matrix has shape torch.Size([11008, 4096])
layer.1.mlp.gate_proj (1002): 0.00807178020477295
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 2725
New matrix has shape torch.Size([11008, 4096])
layer.1.mlp.up_proj (2725): 0.0035526009742170572
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 2839
New matrix has shape torch.Size([4096, 11008])
layer.1.mlp.down_proj (2839): 0.00427625048905611
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 626
New matrix has shape torch.Size([4096, 4096])
layer.2.self_attn.q_proj (626): 0.00915901642292738
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 518
New matrix has shape torch.Size([4096, 4096])
layer.2.self_attn.k_proj (518): 0.010370289906859398
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2458
New matrix has shape torch.Size([4096, 4096])
layer.2.self_attn.v_proj (2458): 0.00209641270339489
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 1611
New matrix has shape torch.Size([4096, 4096])
layer.2.self_attn.o_proj (1611): 0.004315494559705257
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 2024
New matrix has shape torch.Size([11008, 4096])
layer.2.mlp.gate_proj (2024): 0.006041356828063726
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 3869
New matrix has shape torch.Size([11008, 4096])
layer.2.mlp.up_proj (3869): 0.0011049427557736635
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 3378
New matrix has shape torch.Size([4096, 11008])
layer.2.mlp.down_proj (3378): 0.0028644048143178225
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 897
New matrix has shape torch.Size([4096, 4096])
layer.3.self_attn.q_proj (897): 0.008942486718297005
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 795
New matrix has shape torch.Size([4096, 4096])
layer.3.self_attn.k_proj (795): 0.010068411007523537
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2649
New matrix has shape torch.Size([4096, 4096])
layer.3.self_attn.v_proj (2649): 0.0017159414710476995
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 1388
New matrix has shape torch.Size([4096, 4096])
layer.3.self_attn.o_proj (1388): 0.0046878899447619915
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 2461
New matrix has shape torch.Size([11008, 4096])
layer.3.mlp.gate_proj (2461): 0.0055399504490196705
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 3957
New matrix has shape torch.Size([11008, 4096])
layer.3.mlp.up_proj (3957): 0.0008954288205131888
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 3029
New matrix has shape torch.Size([4096, 11008])
layer.3.mlp.down_proj (3029): 0.003610622137784958
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 870
New matrix has shape torch.Size([4096, 4096])
layer.4.self_attn.q_proj (870): 0.009294044226408005
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 748
New matrix has shape torch.Size([4096, 4096])
layer.4.self_attn.k_proj (748): 0.010492983274161816
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2714
New matrix has shape torch.Size([4096, 4096])
layer.4.self_attn.v_proj (2714): 0.00171748420689255
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 1672
New matrix has shape torch.Size([4096, 4096])
layer.4.self_attn.o_proj (1672): 0.0041382042691111565
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 2270
New matrix has shape torch.Size([11008, 4096])
layer.4.mlp.gate_proj (2270): 0.006249851081520319
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 4015
New matrix has shape torch.Size([11008, 4096])
layer.4.mlp.up_proj (4015): 0.0006329351454041898
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 2970
New matrix has shape torch.Size([4096, 11008])
layer.4.mlp.down_proj (2970): 0.003739339765161276
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 896
New matrix has shape torch.Size([4096, 4096])
layer.5.self_attn.q_proj (896): 0.009399019181728363
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 711
New matrix has shape torch.Size([4096, 4096])
layer.5.self_attn.k_proj (711): 0.011154156178236008
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2773
New matrix has shape torch.Size([4096, 4096])
layer.5.self_attn.v_proj (2773): 0.0016058452893048525
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 1749
New matrix has shape torch.Size([4096, 4096])
layer.5.self_attn.o_proj (1749): 0.003923517651855946
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 1718
New matrix has shape torch.Size([11008, 4096])
layer.5.mlp.gate_proj (1718): 0.007908312603831291
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 3925
New matrix has shape torch.Size([11008, 4096])
layer.5.mlp.up_proj (3925): 0.0010671886848285794
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 2993
New matrix has shape torch.Size([4096, 11008])
layer.5.mlp.down_proj (2993): 0.003597851376980543
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 913
New matrix has shape torch.Size([4096, 4096])
layer.6.self_attn.q_proj (913): 0.00848616473376751
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 734
New matrix has shape torch.Size([4096, 4096])
layer.6.self_attn.k_proj (734): 0.00997214112430811
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2625
New matrix has shape torch.Size([4096, 4096])
layer.6.self_attn.v_proj (2625): 0.0016969171119853854
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 1512
New matrix has shape torch.Size([4096, 4096])
layer.6.self_attn.o_proj (1512): 0.004116015508770943
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 1340
New matrix has shape torch.Size([11008, 4096])
layer.6.mlp.gate_proj (1340): 0.009205061942338943
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 3752
New matrix has shape torch.Size([11008, 4096])
layer.6.mlp.up_proj (3752): 0.0016613819170743227
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 3120
New matrix has shape torch.Size([4096, 11008])
layer.6.mlp.down_proj (3120): 0.003206736408174038
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 1005
New matrix has shape torch.Size([4096, 4096])
layer.7.self_attn.q_proj (1005): 0.008084055036306381
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 811
New matrix has shape torch.Size([4096, 4096])
layer.7.self_attn.k_proj (811): 0.009411061182618141
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2548
New matrix has shape torch.Size([4096, 4096])
layer.7.self_attn.v_proj (2548): 0.0017904798733070493
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 1668
New matrix has shape torch.Size([4096, 4096])
layer.7.self_attn.o_proj (1668): 0.003788406727835536
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 1281
New matrix has shape torch.Size([11008, 4096])
layer.7.mlp.gate_proj (1281): 0.00929293967783451
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 3605
New matrix has shape torch.Size([11008, 4096])
layer.7.mlp.up_proj (3605): 0.0020953190978616476
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 2805
New matrix has shape torch.Size([4096, 11008])
layer.7.mlp.down_proj (2805): 0.003989167977124453
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 1020
New matrix has shape torch.Size([4096, 4096])
layer.8.self_attn.q_proj (1020): 0.008284298703074455
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 826
New matrix has shape torch.Size([4096, 4096])
layer.8.self_attn.k_proj (826): 0.009863256476819515
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2447
New matrix has shape torch.Size([4096, 4096])
layer.8.self_attn.v_proj (2447): 0.001991222845390439
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 1875
New matrix has shape torch.Size([4096, 4096])
layer.8.self_attn.o_proj (1875): 0.003310326486825943
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 1426
New matrix has shape torch.Size([11008, 4096])
layer.8.mlp.gate_proj (1426): 0.008600563742220402
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 3487
New matrix has shape torch.Size([11008, 4096])
layer.8.mlp.up_proj (3487): 0.0024576964788138866
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 2685
New matrix has shape torch.Size([4096, 11008])
layer.8.mlp.down_proj (2685): 0.004322872497141361
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 1038
New matrix has shape torch.Size([4096, 4096])
layer.9.self_attn.q_proj (1038): 0.00854785367846489
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 865
New matrix has shape torch.Size([4096, 4096])
layer.9.self_attn.k_proj (865): 0.0099581778049469
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2524
New matrix has shape torch.Size([4096, 4096])
layer.9.self_attn.v_proj (2524): 0.0018627454992383718
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 1841
New matrix has shape torch.Size([4096, 4096])
layer.9.self_attn.o_proj (1841): 0.0034561697393655777
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 1373
New matrix has shape torch.Size([11008, 4096])
layer.9.mlp.gate_proj (1373): 0.008654648438096046
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 3450
New matrix has shape torch.Size([11008, 4096])
layer.9.mlp.up_proj (3450): 0.0025604809634387493
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 2620
New matrix has shape torch.Size([4096, 11008])
layer.9.mlp.down_proj (2620): 0.004530774895101786
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 981
New matrix has shape torch.Size([4096, 4096])
layer.10.self_attn.q_proj (981): 0.008298654109239578
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 818
New matrix has shape torch.Size([4096, 4096])
layer.10.self_attn.k_proj (818): 0.009841890074312687
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2506
New matrix has shape torch.Size([4096, 4096])
layer.10.self_attn.v_proj (2506): 0.0018780588870868087
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2045
New matrix has shape torch.Size([4096, 4096])
layer.10.self_attn.o_proj (2045): 0.0027440846897661686
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 1329
New matrix has shape torch.Size([11008, 4096])
layer.10.mlp.gate_proj (1329): 0.008687852881848812
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 3429
New matrix has shape torch.Size([11008, 4096])
layer.10.mlp.up_proj (3429): 0.0026342207565903664
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 2904
New matrix has shape torch.Size([4096, 11008])
layer.10.mlp.down_proj (2904): 0.003774561220780015
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 968
New matrix has shape torch.Size([4096, 4096])
layer.11.self_attn.q_proj (968): 0.00765674002468586
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 802
New matrix has shape torch.Size([4096, 4096])
layer.11.self_attn.k_proj (802): 0.008886477909982204
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2439
New matrix has shape torch.Size([4096, 4096])
layer.11.self_attn.v_proj (2439): 0.002002085093408823
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 1616
New matrix has shape torch.Size([4096, 4096])
layer.11.self_attn.o_proj (1616): 0.0038545236457139254
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 1257
New matrix has shape torch.Size([11008, 4096])
layer.11.mlp.gate_proj (1257): 0.008909876458346844
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 3422
New matrix has shape torch.Size([11008, 4096])
layer.11.mlp.up_proj (3422): 0.002638466190546751
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 2591
New matrix has shape torch.Size([4096, 11008])
layer.11.mlp.down_proj (2591): 0.004702115431427956
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 1033
New matrix has shape torch.Size([4096, 4096])
layer.12.self_attn.q_proj (1033): 0.008257218636572361
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 851
New matrix has shape torch.Size([4096, 4096])
layer.12.self_attn.k_proj (851): 0.009580861777067184
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2552
New matrix has shape torch.Size([4096, 4096])
layer.12.self_attn.v_proj (2552): 0.0018485327018424869
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 1926
New matrix has shape torch.Size([4096, 4096])
layer.12.self_attn.o_proj (1926): 0.0032625312451273203
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 1394
New matrix has shape torch.Size([11008, 4096])
layer.12.mlp.gate_proj (1394): 0.008454852737486362
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 3399
New matrix has shape torch.Size([11008, 4096])
layer.12.mlp.up_proj (3399): 0.0027177368756383657
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 3097
New matrix has shape torch.Size([4096, 11008])
layer.12.mlp.down_proj (3097): 0.003371495520696044
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 983
New matrix has shape torch.Size([4096, 4096])
layer.13.self_attn.q_proj (983): 0.008543326519429684
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 849
New matrix has shape torch.Size([4096, 4096])
layer.13.self_attn.k_proj (849): 0.009872669354081154
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2732
New matrix has shape torch.Size([4096, 4096])
layer.13.self_attn.v_proj (2732): 0.0016879900358617306
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 1906
New matrix has shape torch.Size([4096, 4096])
layer.13.self_attn.o_proj (1906): 0.003362616989761591
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 1372
New matrix has shape torch.Size([11008, 4096])
layer.13.mlp.gate_proj (1372): 0.00856122188270092
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 3314
New matrix has shape torch.Size([11008, 4096])
layer.13.mlp.up_proj (3314): 0.003021280048415065
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 3058
New matrix has shape torch.Size([4096, 11008])
layer.13.mlp.down_proj (3058): 0.003602784126996994
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 974
New matrix has shape torch.Size([4096, 4096])
layer.14.self_attn.q_proj (974): 0.008197864517569542
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 766
New matrix has shape torch.Size([4096, 4096])
layer.14.self_attn.k_proj (766): 0.00968797504901886
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2834
New matrix has shape torch.Size([4096, 4096])
layer.14.self_attn.v_proj (2834): 0.0014365259557962418
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2071
New matrix has shape torch.Size([4096, 4096])
layer.14.self_attn.o_proj (2071): 0.0028918241150677204
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 1399
New matrix has shape torch.Size([11008, 4096])
layer.14.mlp.gate_proj (1399): 0.008472312241792679
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 3276
New matrix has shape torch.Size([11008, 4096])
layer.14.mlp.up_proj (3276): 0.0031068511307239532
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 3189
New matrix has shape torch.Size([4096, 11008])
layer.14.mlp.down_proj (3189): 0.0032561994157731533
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 884
New matrix has shape torch.Size([4096, 4096])
layer.15.self_attn.q_proj (884): 0.009037991054356098
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 743
New matrix has shape torch.Size([4096, 4096])
layer.15.self_attn.k_proj (743): 0.010263903066515923
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2813
New matrix has shape torch.Size([4096, 4096])
layer.15.self_attn.v_proj (2813): 0.0015891059301793575
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2175
New matrix has shape torch.Size([4096, 4096])
layer.15.self_attn.o_proj (2175): 0.0028783613815903664
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 1415
New matrix has shape torch.Size([11008, 4096])
layer.15.mlp.gate_proj (1415): 0.008536167442798615
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 3201
New matrix has shape torch.Size([11008, 4096])
layer.15.mlp.up_proj (3201): 0.0034071397967636585
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 3378
New matrix has shape torch.Size([4096, 11008])
layer.15.mlp.down_proj (3378): 0.002838956192135811
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 818
New matrix has shape torch.Size([4096, 4096])
layer.16.self_attn.q_proj (818): 0.009239221923053265
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 681
New matrix has shape torch.Size([4096, 4096])
layer.16.self_attn.k_proj (681): 0.010448074899613857
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2985
New matrix has shape torch.Size([4096, 4096])
layer.16.self_attn.v_proj (2985): 0.001405925489962101
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2202
New matrix has shape torch.Size([4096, 4096])
layer.16.self_attn.o_proj (2202): 0.0031447415240108967
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 1398
New matrix has shape torch.Size([11008, 4096])
layer.16.mlp.gate_proj (1398): 0.008635365404188633
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 3171
New matrix has shape torch.Size([11008, 4096])
layer.16.mlp.up_proj (3171): 0.003427149262279272
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 3517
New matrix has shape torch.Size([4096, 11008])
layer.16.mlp.down_proj (3517): 0.002468913560733199
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 901
New matrix has shape torch.Size([4096, 4096])
layer.17.self_attn.q_proj (901): 0.009445575065910816
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 740
New matrix has shape torch.Size([4096, 4096])
layer.17.self_attn.k_proj (740): 0.010592430830001831
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2999
New matrix has shape torch.Size([4096, 4096])
layer.17.self_attn.v_proj (2999): 0.0014286958612501621
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2118
New matrix has shape torch.Size([4096, 4096])
layer.17.self_attn.o_proj (2118): 0.003538993652909994
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 1336
New matrix has shape torch.Size([11008, 4096])
layer.17.mlp.gate_proj (1336): 0.008966644294559956
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 3368
New matrix has shape torch.Size([11008, 4096])
layer.17.mlp.up_proj (3368): 0.002829959848895669
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 3594
New matrix has shape torch.Size([4096, 11008])
layer.17.mlp.down_proj (3594): 0.0022043983917683363
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 898
New matrix has shape torch.Size([4096, 4096])
layer.18.self_attn.q_proj (898): 0.00967482291162014
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 795
New matrix has shape torch.Size([4096, 4096])
layer.18.self_attn.k_proj (795): 0.01059512235224247
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 3055
New matrix has shape torch.Size([4096, 4096])
layer.18.self_attn.v_proj (3055): 0.001388695789501071
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2242
New matrix has shape torch.Size([4096, 4096])
layer.18.self_attn.o_proj (2242): 0.0034562107175588608
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 1516
New matrix has shape torch.Size([11008, 4096])
layer.18.mlp.gate_proj (1516): 0.008547049947082996
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 3452
New matrix has shape torch.Size([11008, 4096])
layer.18.mlp.up_proj (3452): 0.002599418396130204
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 3622
New matrix has shape torch.Size([4096, 11008])
layer.18.mlp.down_proj (3622): 0.0021459744311869144
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 866
New matrix has shape torch.Size([4096, 4096])
layer.19.self_attn.q_proj (866): 0.009437792003154755
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 704
New matrix has shape torch.Size([4096, 4096])
layer.19.self_attn.k_proj (704): 0.01053600199520588
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 3094
New matrix has shape torch.Size([4096, 4096])
layer.19.self_attn.v_proj (3094): 0.0012953821569681168
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2278
New matrix has shape torch.Size([4096, 4096])
layer.19.self_attn.o_proj (2278): 0.0033834711648523808
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 1705
New matrix has shape torch.Size([11008, 4096])
layer.19.mlp.gate_proj (1705): 0.007990705780684948
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 3501
New matrix has shape torch.Size([11008, 4096])
layer.19.mlp.up_proj (3501): 0.002467866986989975
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 3570
New matrix has shape torch.Size([4096, 11008])
layer.19.mlp.down_proj (3570): 0.002334216609597206
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 799
New matrix has shape torch.Size([4096, 4096])
layer.20.self_attn.q_proj (799): 0.009720261208713055
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 719
New matrix has shape torch.Size([4096, 4096])
layer.20.self_attn.k_proj (719): 0.010419435799121857
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 3124
New matrix has shape torch.Size([4096, 4096])
layer.20.self_attn.v_proj (3124): 0.0012799811083823442
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 1997
New matrix has shape torch.Size([4096, 4096])
layer.20.self_attn.o_proj (1997): 0.004467403050512075
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 1768
New matrix has shape torch.Size([11008, 4096])
layer.20.mlp.gate_proj (1768): 0.0078062815591692924
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 3513
New matrix has shape torch.Size([11008, 4096])
layer.20.mlp.up_proj (3513): 0.0024128274526447058
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 3717
New matrix has shape torch.Size([4096, 11008])
layer.20.mlp.down_proj (3717): 0.0018792706541717052
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 864
New matrix has shape torch.Size([4096, 4096])
layer.21.self_attn.q_proj (864): 0.009363910183310509
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 819
New matrix has shape torch.Size([4096, 4096])
layer.21.self_attn.k_proj (819): 0.00978922750800848
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2986
New matrix has shape torch.Size([4096, 4096])
layer.21.self_attn.v_proj (2986): 0.0015958163421601057
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2337
New matrix has shape torch.Size([4096, 4096])
layer.21.self_attn.o_proj (2337): 0.0035393070429563522
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 1922
New matrix has shape torch.Size([11008, 4096])
layer.21.mlp.gate_proj (1922): 0.007369400467723608
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 3594
New matrix has shape torch.Size([11008, 4096])
layer.21.mlp.up_proj (3594): 0.002163937781006098
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 3763
New matrix has shape torch.Size([4096, 11008])
layer.21.mlp.down_proj (3763): 0.0017048574518412352
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 912
New matrix has shape torch.Size([4096, 4096])
layer.22.self_attn.q_proj (912): 0.0095176100730896
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 885
New matrix has shape torch.Size([4096, 4096])
layer.22.self_attn.k_proj (885): 0.009886162355542183
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 3188
New matrix has shape torch.Size([4096, 4096])
layer.22.self_attn.v_proj (3188): 0.0012292570900171995
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 1776
New matrix has shape torch.Size([4096, 4096])
layer.22.self_attn.o_proj (1776): 0.0054451171308755875
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 1962
New matrix has shape torch.Size([11008, 4096])
layer.22.mlp.gate_proj (1962): 0.0072939395904541016
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 3692
New matrix has shape torch.Size([11008, 4096])
layer.22.mlp.up_proj (3692): 0.0018579467432573438
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 3653
New matrix has shape torch.Size([4096, 11008])
layer.22.mlp.down_proj (3653): 0.002059205435216427
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 1037
New matrix has shape torch.Size([4096, 4096])
layer.23.self_attn.q_proj (1037): 0.009307058528065681
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 990
New matrix has shape torch.Size([4096, 4096])
layer.23.self_attn.k_proj (990): 0.009646516293287277
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 3137
New matrix has shape torch.Size([4096, 4096])
layer.23.self_attn.v_proj (3137): 0.0013982037780806422
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2436
New matrix has shape torch.Size([4096, 4096])
layer.23.self_attn.o_proj (2436): 0.0033654901199042797
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 2276
New matrix has shape torch.Size([11008, 4096])
layer.23.mlp.gate_proj (2276): 0.00634413817897439
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 3810
New matrix has shape torch.Size([11008, 4096])
layer.23.mlp.up_proj (3810): 0.0014927900629118085
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 3490
New matrix has shape torch.Size([4096, 11008])
layer.23.mlp.down_proj (3490): 0.0025576443877071142
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 900
New matrix has shape torch.Size([4096, 4096])
layer.24.self_attn.q_proj (900): 0.008906373754143715
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 809
New matrix has shape torch.Size([4096, 4096])
layer.24.self_attn.k_proj (809): 0.009555334225296974
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 3054
New matrix has shape torch.Size([4096, 4096])
layer.24.self_attn.v_proj (3054): 0.0015497073763981462
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2105
New matrix has shape torch.Size([4096, 4096])
layer.24.self_attn.o_proj (2105): 0.004456011112779379
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 2312
New matrix has shape torch.Size([11008, 4096])
layer.24.mlp.gate_proj (2312): 0.00628242501989007
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 3873
New matrix has shape torch.Size([11008, 4096])
layer.24.mlp.up_proj (3873): 0.0012774976203218102
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 3558
New matrix has shape torch.Size([4096, 11008])
layer.24.mlp.down_proj (3558): 0.002332558622583747
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 1081
New matrix has shape torch.Size([4096, 4096])
layer.25.self_attn.q_proj (1081): 0.008756236173212528
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 1039
New matrix has shape torch.Size([4096, 4096])
layer.25.self_attn.k_proj (1039): 0.009166577830910683
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 3123
New matrix has shape torch.Size([4096, 4096])
layer.25.self_attn.v_proj (3123): 0.0014848409919068217
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2632
New matrix has shape torch.Size([4096, 4096])
layer.25.self_attn.o_proj (2632): 0.0027809832245111465
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 2338
New matrix has shape torch.Size([11008, 4096])
layer.25.mlp.gate_proj (2338): 0.006268346216529608
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 3806
New matrix has shape torch.Size([11008, 4096])
layer.25.mlp.up_proj (3806): 0.001557628740556538
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 3622
New matrix has shape torch.Size([4096, 11008])
layer.25.mlp.down_proj (3622): 0.002124043181538582
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 825
New matrix has shape torch.Size([4096, 4096])
layer.26.self_attn.q_proj (825): 0.009485946968197823
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 800
New matrix has shape torch.Size([4096, 4096])
layer.26.self_attn.k_proj (800): 0.009879679419100285
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2977
New matrix has shape torch.Size([4096, 4096])
layer.26.self_attn.v_proj (2977): 0.001819041557610035
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2121
New matrix has shape torch.Size([4096, 4096])
layer.26.self_attn.o_proj (2121): 0.0045279026962816715
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 2172
New matrix has shape torch.Size([11008, 4096])
layer.26.mlp.gate_proj (2172): 0.006855602841824293
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 3626
New matrix has shape torch.Size([11008, 4096])
layer.26.mlp.up_proj (3626): 0.0022082652430981398
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 3681
New matrix has shape torch.Size([4096, 11008])
layer.26.mlp.down_proj (3681): 0.0019508107798174024
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 941
New matrix has shape torch.Size([4096, 4096])
layer.27.self_attn.q_proj (941): 0.010107547976076603
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 845
New matrix has shape torch.Size([4096, 4096])
layer.27.self_attn.k_proj (845): 0.010927150025963783
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 3125
New matrix has shape torch.Size([4096, 4096])
layer.27.self_attn.v_proj (3125): 0.0015516788698732853
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2460
New matrix has shape torch.Size([4096, 4096])
layer.27.self_attn.o_proj (2460): 0.003432678058743477
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 2171
New matrix has shape torch.Size([11008, 4096])
layer.27.mlp.gate_proj (2171): 0.006918793544173241
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 3351
New matrix has shape torch.Size([11008, 4096])
layer.27.mlp.up_proj (3351): 0.0031077696476131678
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 3625
New matrix has shape torch.Size([4096, 11008])
layer.27.mlp.down_proj (3625): 0.0021481418516486883
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 915
New matrix has shape torch.Size([4096, 4096])
layer.28.self_attn.q_proj (915): 0.009638245217502117
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 844
New matrix has shape torch.Size([4096, 4096])
layer.28.self_attn.k_proj (844): 0.010362206026911736
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 3042
New matrix has shape torch.Size([4096, 4096])
layer.28.self_attn.v_proj (3042): 0.0017955107614398003
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2364
New matrix has shape torch.Size([4096, 4096])
layer.28.self_attn.o_proj (2364): 0.003772832453250885
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 2140
New matrix has shape torch.Size([11008, 4096])
layer.28.mlp.gate_proj (2140): 0.006990314926952124
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 2781
New matrix has shape torch.Size([11008, 4096])
layer.28.mlp.up_proj (2781): 0.004861315246671438
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 3696
New matrix has shape torch.Size([4096, 11008])
layer.28.mlp.down_proj (3696): 0.0019895241130143404
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 723
New matrix has shape torch.Size([4096, 4096])
layer.29.self_attn.q_proj (723): 0.009865274652838707
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 643
New matrix has shape torch.Size([4096, 4096])
layer.29.self_attn.k_proj (643): 0.010611391626298428
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2722
New matrix has shape torch.Size([4096, 4096])
layer.29.self_attn.v_proj (2722): 0.0026486138813197613
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2215
New matrix has shape torch.Size([4096, 4096])
layer.29.self_attn.o_proj (2215): 0.004417330492287874
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 1953
New matrix has shape torch.Size([11008, 4096])
layer.29.mlp.gate_proj (1953): 0.007618089206516743
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 2134
New matrix has shape torch.Size([11008, 4096])
layer.29.mlp.up_proj (2134): 0.006855145562440157
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 3802
New matrix has shape torch.Size([4096, 11008])
layer.29.mlp.down_proj (3802): 0.0016709648771211505
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 805
New matrix has shape torch.Size([4096, 4096])
layer.30.self_attn.q_proj (805): 0.010060945525765419
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 753
New matrix has shape torch.Size([4096, 4096])
layer.30.self_attn.k_proj (753): 0.010664094239473343
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 3064
New matrix has shape torch.Size([4096, 4096])
layer.30.self_attn.v_proj (3064): 0.0018197335302829742
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2257
New matrix has shape torch.Size([4096, 4096])
layer.30.self_attn.o_proj (2257): 0.004400079138576984
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 487
New matrix has shape torch.Size([11008, 4096])
layer.30.mlp.gate_proj (487): 0.01318755280226469
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 312
New matrix has shape torch.Size([11008, 4096])
layer.30.mlp.up_proj (312): 0.013403953053057194
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 3637
New matrix has shape torch.Size([4096, 11008])
layer.30.mlp.down_proj (3637): 0.002246300922706723
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 471
New matrix has shape torch.Size([4096, 4096])
layer.31.self_attn.q_proj (471): 0.01102712657302618
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 419
New matrix has shape torch.Size([4096, 4096])
layer.31.self_attn.k_proj (419): 0.012010355480015278
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 2847
New matrix has shape torch.Size([4096, 4096])
layer.31.self_attn.v_proj (2847): 0.0021130573004484177
Shape is torch.Size([4096, 4096]) and shape is torch.float32 => desired rank 1140
New matrix has shape torch.Size([4096, 4096])
layer.31.self_attn.o_proj (1140): 0.008171692490577698
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 433
New matrix has shape torch.Size([11008, 4096])
layer.31.mlp.gate_proj (433): 0.013733870349824429
Shape is torch.Size([11008, 4096]) and shape is torch.float32 => desired rank 233
New matrix has shape torch.Size([11008, 4096])
layer.31.mlp.up_proj (233): 0.013990920968353748
Shape is torch.Size([4096, 11008]) and shape is torch.float32 => desired rank 2541
New matrix has shape torch.Size([4096, 11008])
layer.31.mlp.down_proj (2541): 0.005353954154998064
Pruning completed
evaluating on wikitext2
nsamples 83
sample 0/83
sample 1/83
sample 2/83
sample 3/83
sample 4/83
sample 5/83
sample 6/83
sample 7/83
sample 8/83
sample 9/83
sample 10/83
sample 11/83
sample 12/83
sample 13/83
sample 14/83
sample 15/83
sample 16/83
sample 17/83
sample 18/83
sample 19/83
sample 20/83
sample 21/83
sample 22/83
sample 23/83
sample 24/83
sample 25/83
sample 26/83
sample 27/83
sample 28/83
sample 29/83
sample 30/83
sample 31/83
sample 32/83
sample 33/83
sample 34/83
sample 35/83
sample 36/83
sample 37/83
sample 38/83
sample 39/83
sample 40/83
sample 41/83
sample 42/83
sample 43/83
sample 44/83
sample 45/83
sample 46/83
sample 47/83
sample 48/83
sample 49/83
sample 50/83
sample 51/83
sample 52/83
sample 53/83
sample 54/83
sample 55/83
sample 56/83
sample 57/83
sample 58/83
sample 59/83
sample 60/83
sample 61/83
sample 62/83
sample 63/83
sample 64/83
sample 65/83
sample 66/83
sample 67/83
sample 68/83
sample 69/83
sample 70/83
sample 71/83
sample 72/83
sample 73/83
sample 74/83
sample 75/83
sample 76/83
sample 77/83
sample 78/83
sample 79/83
sample 80/83
sample 81/83
sample 82/83
B  
C  
D  
B C
B  
A C
A M
D C
B  
C  
C M
C M
A  
C  
C  
B  
C  
C e
D M
A  
A A
A  
D C
D n
B A
C H
C C
B C
D  
A C
B H
B M
A C
C H
A C
B  
D  
B M
C  
D C
C  
A  
B  
C C
C C
C A
B A
B  
C  
B  
D C
B  
A  
C C
B  
A C
D  
B  
A C
A H
D  
D C
C  
B A
B  
C C
D C
C M
A  
A C
D  
C  
D  
B  
D C
B C
C  
A  
D C
C C
B C
D  
A C
C  
D A
C  
A C
A  
A  
C C
D C
B  
B  
A  
D C
C C
B  
C  
C  
C C
A  
B A
A Z
C n
B  
B Z
A  
B C
C n
D M
D  
D M
D  
B M
A n
C  
B r
